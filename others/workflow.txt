
通信第t轮：
在训练每一轮的时候，你进行完参数平均后，就要把self.global_params发给所有的客户端，然后进行测试，再接着进行训练



Thinking 问题：

自己写一遍框架，发现有很多细节没有注意

i.全局把最新的模型参数发给所有的客户端，然后让他们进行eval，而训练只是发给被选中的客户端

ii.MNIST训练集是如何划分的呢？每个框架异构数据集的划分不太一样啊

iii.更新参数的时候只是用当前这K个参数进行加权平均，那如果这一轮选的K个客户端有几个是坏的参数，那不是直接把模型给搞坏了吗？（及时前面50轮你选的所有客户端都是正常的）
那么问题来了，怎么去判断一个客户端是好还是坏的，因为一个参数就算本地

iv.正常来讲我们是希望每个客户端的本地性能都不错，但是现在我们做的实验室测他们的平均性能，也就是认为如果全局模型性能好，那每个客户端性能平均来看还是不错的。
    实际上我们可以让数据集多的客户端的精度更准确，之前吴yh讲的q-fedavg就是在考虑这点

v.每轮只会选几个客户端进行训练，而所有客户端都会参与测试（这个只是用来看一下效果怎么样，跟训练没关系），所以你隔5/10轮测一次都行，测试只是显示训练的优化过程而已。在测试阶段客户端断线是没有影响的，我只是不知道精确的准确率而已，不影响训练过程。而在训练截断客户端断线还是有影响的，这会直接影响训练，即参数的优化。


vi. seed会在哪几个地方用？
    1. 随机选择客户端
    2. 模型初始化 + dataloader seed
    # 使模型随机性+dataloader shuffle不具有随机性
    # 随机选择客户端的随机性要动态的

Q:为什么代码上传到服务器后用远程的编译器运行就非常慢？
A: 原来是有人在跑很多的实验把cpu和gpu占用满了


21/3/23
今天尝试了用自己的框架跑了下MNIST数据集，跑的是集中式的，我想看看自己的框架在标准数据集上有没有问题。一开始我没注意到我的学习率设的是0.03，有点大了，然后测了几轮，准确率只有10%左右，训练集上一个batch的loss基本都在2.3到3.7之间跳动。

后来我尝试了下把学习率调成0.003，maya，我发现loss瞬间就下来了，直接到0.0几了，而且测了一轮，准确率就到98.5%了，说明啊，我之前设的步长太大了。导致没法根本没法往下走。所以loss不降。

问了下tdye，它说可以设一下学习率衰减，一开始大一点，后面变小。


21/3/30
我们探讨了下state_dict是shallow copy对模型训练是否有影响，已经load_state_dict是deep copy

叶之前保存最优模型用的也是state_dict()，后面他修改成deepcopy了

--------------------------------

21/4/5
1. 降低我框架的耦合性，每个模块只完成一个功能，然后以接口地地方被调用，尽量达到解耦的理想状态（让程序变成搭积木化，相互之间尽量独立）
我一个federate函数完成的功能是在太多了，要分开了，可以参考kunkun的lightfed的写法

2. 把movielens处理好，放到我的框架中跑一跑



--------------------------------
需要思考的问题：
1. 集中式和分布式对比时，communication rounds是否要一样，一样fedavg是要进行多次本地更新的？



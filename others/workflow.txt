
通信第t轮：
在训练每一轮的时候，你进行完参数平均后，就要把self.global_params发给所有的客户端，然后进行测试，再接着进行训练



Thinking 问题：

自己写一遍框架，发现有很多细节没有注意

i.全局把最新的模型参数发给所有的客户端，然后让他们进行eval，而训练只是发给被选中的客户端

ii.MNIST训练集是如何划分的呢？每个框架异构数据集的划分不太一样啊

iii.更新参数的时候只是用当前这K个参数进行加权平均，那如果这一轮选的K个客户端有几个是坏的参数，那不是直接把模型给搞坏了吗？（及时前面50轮你选的所有客户端都是正常的）
那么问题来了，怎么去判断一个客户端是好还是坏的，因为一个参数就算本地

iv.正常来讲我们是希望每个客户端的本地性能都不错，但是现在我们做的实验室测他们的平均性能，也就是认为如果全局模型性能好，那每个客户端性能平均来看还是不错的。
    实际上我们可以让数据集多的客户端的精度更准确，之前吴yh讲的q-fedavg就是在考虑这点

v.每轮只会选几个客户端进行训练，而所有客户端都会参与测试（这个只是用来看一下效果怎么样，跟训练没关系），所以你隔5/10轮测一次都行，测试只是显示训练的优化过程而已。在测试阶段客户端断线是没有影响的，我只是不知道精确的准确率而已，不影响训练过程。而在训练截断客户端断线还是有影响的，这会直接影响训练，即参数的优化。


vi. seed会在哪几个地方用？
    1. 随机选择客户端
    2. 模型初始化 + dataloader seed
    # 使模型随机性+dataloader shuffle不具有随机性
    # 随机选择客户端的随机性要动态的

Q:为什么代码上传到服务器后用远程的编译器运行就非常慢？
A: 原来是有人在跑很多的实验把cpu和gpu占用满了

